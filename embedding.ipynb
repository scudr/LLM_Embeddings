{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento de Texto con spaCy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto limpio: ejemplo texto preprocesar\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Cargar el modelo de spaCy para preprocesamiento\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Funci√≥n de preprocesamiento\n",
    "def preprocess_text(text:str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocesa el texto eliminando stopwords y puntuaci√≥n, y lematiza las palabras.\n",
    "\n",
    "    Args:\n",
    "        text (str): Texto a preprocesar.\n",
    "\n",
    "    Returns:\n",
    "        str: Texto preprocesado.\n",
    "    \"\"\"\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct])\n",
    "\n",
    "# Texto de ejemplo\n",
    "text = \"Este es un ejemplo de texto para preprocesar.\"\n",
    "cleaned_text = preprocess_text(text)\n",
    "print(\"Texto limpio:\", cleaned_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtener Embeddings con DistilBERT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings: tensor([[[-0.3835, -0.0783, -0.0687,  ..., -0.0525,  0.2811,  0.5119],\n",
      "         [-0.3267, -0.2704,  0.0471,  ...,  0.1809,  0.3451,  0.5138],\n",
      "         [-0.5358, -0.4345,  0.1403,  ...,  0.0510, -0.0980,  0.6037],\n",
      "         ...,\n",
      "         [ 0.2616, -0.1185,  0.2342,  ...,  0.0457, -0.0370,  0.2346],\n",
      "         [-0.3324, -0.0216, -0.1785,  ...,  0.0930, -0.0885,  0.2521],\n",
      "         [ 0.7774,  0.0768, -0.3888,  ...,  0.2486, -0.7300, -0.1771]]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "\n",
    "# Configurar el dispositivo (GPU si est√° disponible)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Tokenizador y modelo de DistilBERT\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)\n",
    "\n",
    "# Funci√≥n para obtener embeddings de DistilBERT\n",
    "def get_embeddings(text:str, model:DistilBertModel, tokenizer:DistilBertTokenizer) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Genera los embeddings para el texto dado utilizando el modelo DistilBERT.\n",
    "\n",
    "    Args:\n",
    "        text (str): Texto para el cual generar los embeddings.\n",
    "        model (DistilBertModel): Modelo DistilBERT preentrenado.\n",
    "        tokenizer (DistilBertTokenizer): Tokenizador DistilBERT preentrenado.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Embeddings generados por el modelo DistilBERT.\n",
    "    \"\"\"\n",
    "    encoded_input = tokenizer(text, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoded_input)\n",
    "    return outputs.last_hidden_state\n",
    "\n",
    "# Obtener embeddings para el texto preprocesado\n",
    "embeddings = get_embeddings(cleaned_text, model, tokenizer)\n",
    "print(\"Embeddings:\", embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Calcula las m√©tricas de evaluaci√≥n para el modelo.\n",
    "\n",
    "    Args:\n",
    "        eval_pred (tuple): Una tupla que contiene logits y etiquetas reales.\n",
    "\n",
    "    Returns:\n",
    "        dict: Un diccionario con la m√©trica calculada (precisi√≥n en este caso).\n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    metric = load_metric(\"accuracy\", trust_remote_code=True)\n",
    "    accuracy = metric.compute(predictions=predictions, references=labels)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga y Evaluaci√≥n del Modelo sin Fine-Tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\cajue\\anaconda3\\envs\\cibertec--llm\\Lib\\site-packages\\accelerate\\accelerator.py:482: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:03<00:00, 82.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados del modelo sin fine-tuning: {'eval_loss': 0.7018254399299622, 'eval_accuracy': 0.4995, 'eval_runtime': 3.1329, 'eval_samples_per_second': 638.384, 'eval_steps_per_second': 79.798}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer\n",
    "\n",
    "# Cargar el modelo preentrenado sin fine-tuning\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)  # Ajusta el n√∫mero de etiquetas seg√∫n tu caso\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Definir los argumentos de entrenamiento (necesarios para el Trainer)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    per_device_eval_batch_size=8,\n",
    "    do_train=False,\n",
    "    do_eval=True,\n",
    ")\n",
    "\n",
    "# Crear un Trainer para evaluar el modelo preentrenado\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics  # Incluir la funci√≥n de m√©trica personalizada\n",
    ")\n",
    "\n",
    "# Evaluar el modelo preentrenado\n",
    "baseline_results = trainer.evaluate()\n",
    "print(\"Resultados del modelo sin fine-tuning:\", baseline_results)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Importaci√≥n de Librer√≠as y Configuraci√≥n del Logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Configurar el logger para redirigir la salida a un archivo y establecer el nivel de logging a WARNING\n",
    "logging.basicConfig(filename='training_log.txt', level=logging.WARNING)\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Silenciar advertencias espec√≠ficas de PyTorch\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='torch')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargar y Preprocesar el Dataset de IMDb en Espa√±ol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset de IMDb en espa√±ol\n",
    "data = pd.read_csv('IMDB_Dataset_SPANISH.csv')\n",
    "\n",
    "# Subsamplear el dataset a 20000 muestras\n",
    "data = data.sample(n=20000, random_state=42)\n",
    "\n",
    "# Mapear etiquetas de texto a valores num√©ricos\n",
    "label_mapping = {\"positivo\": 1, \"negativo\": 0}\n",
    "data['sentimiento'] = data['sentimiento'].map(label_mapping)\n",
    "\n",
    "# Dividir en entrenamiento y validaci√≥n\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(data['review_es'], data['sentimiento'], test_size=0.2, random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizaci√≥n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "# Tokenizaci√≥n\n",
    "tokenizer = BertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts.tolist(), truncation=True, padding=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creaci√≥n del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Definir la clase Dataset\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Seleccionar el dispositivo (GPU si est√° disponible)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Crear los datasets de entrenamiento y validaci√≥n\n",
    "train_dataset = Dataset(train_encodings, train_labels.tolist())\n",
    "val_dataset = Dataset(val_encodings, val_labels.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Cargar el Modelo Preentrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuraci√≥n y Entrenamiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cajue\\anaconda3\\envs\\cibertec--llm\\Lib\\site-packages\\transformers\\training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=2,  # Puedes ajustar seg√∫n los resultados\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.1,  # Aumentar weight decay para evitar sobreajuste\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=1000,\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    eval_steps=1000,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    disable_tqdm=False\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementaci√≥n de un Callback Personalizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        # Solo imprime logs cada cierto n√∫mero de pasos\n",
    "        if state.global_step % 1000 == 0:\n",
    "            print(logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cajue\\anaconda3\\envs\\cibertec--llm\\Lib\\site-packages\\accelerate\\accelerator.py:482: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1000/2000 [01:07<01:07, 14.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5446, 'grad_norm': 2.492856740951538, 'learning_rate': 3.336666666666667e-05, 'epoch': 1.0}\n",
      "{'loss': 0.5446, 'grad_norm': 2.492856740951538, 'learning_rate': 3.336666666666667e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1000/2000 [01:13<01:07, 14.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4171757698059082, 'eval_accuracy': 0.82175, 'eval_runtime': 5.3641, 'eval_samples_per_second': 745.695, 'eval_steps_per_second': 46.606, 'epoch': 1.0}\n",
      "{'eval_loss': 0.4171757698059082, 'eval_accuracy': 0.82175, 'eval_runtime': 5.3641, 'eval_samples_per_second': 745.695, 'eval_steps_per_second': 46.606, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [02:21<00:00, 14.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3138, 'grad_norm': 5.647911071777344, 'learning_rate': 3.3333333333333334e-08, 'epoch': 2.0}\n",
      "{'loss': 0.3138, 'grad_norm': 5.647911071777344, 'learning_rate': 3.3333333333333334e-08, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [02:27<00:00, 14.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.35327568650245667, 'eval_accuracy': 0.85425, 'eval_runtime': 6.306, 'eval_samples_per_second': 634.321, 'eval_steps_per_second': 39.645, 'epoch': 2.0}\n",
      "{'eval_loss': 0.35327568650245667, 'eval_accuracy': 0.85425, 'eval_runtime': 6.306, 'eval_samples_per_second': 634.321, 'eval_steps_per_second': 39.645, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [02:28<00:00, 13.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 148.0249, 'train_samples_per_second': 216.18, 'train_steps_per_second': 13.511, 'total_flos': 4238956756992000.0, 'train_loss': 0.42916552734375, 'epoch': 2.0}\n",
      "{'train_runtime': 148.0249, 'train_samples_per_second': 216.18, 'train_steps_per_second': 13.511, 'train_loss': 0.42916552734375, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2000, training_loss=0.42916552734375, metrics={'train_runtime': 148.0249, 'train_samples_per_second': 216.18, 'train_steps_per_second': 13.511, 'total_flos': 4238956756992000.0, 'train_loss': 0.42916552734375, 'epoch': 2.0})"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definir el entrenador\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,  # A√±adir la funci√≥n de m√©trica personalizada\n",
    "    callbacks=[CustomCallback()]  # Mant√©n esto si tienes callbacks personalizados\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluaci√≥n y Guardado del Modelo Entrenado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:07<00:00, 34.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.35327568650245667, 'eval_accuracy': 0.85425, 'eval_runtime': 7.2679, 'eval_samples_per_second': 550.368, 'eval_steps_per_second': 34.398, 'epoch': 2.0}\n",
      "Resultados de la evaluaci√≥n: {'eval_loss': 0.35327568650245667, 'eval_accuracy': 0.85425, 'eval_runtime': 7.2679, 'eval_samples_per_second': 550.368, 'eval_steps_per_second': 34.398, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./sentiment_model\\\\tokenizer_config.json',\n",
       " './sentiment_model\\\\special_tokens_map.json',\n",
       " './sentiment_model\\\\vocab.txt',\n",
       " './sentiment_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluaci√≥n del modelo\n",
    "eval_result = trainer.evaluate()\n",
    "print(f\"Resultados de la evaluaci√≥n: {eval_result}\")\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "model.save_pretrained('./sentiment_model')\n",
    "tokenizer.save_pretrained('./sentiment_model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicci√≥n de Sentimientos con el Modelo Ajustado\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_1', 'score': 0.9876910448074341}, {'label': 'LABEL_0', 'score': 0.9902077317237854}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "import torch\n",
    "\n",
    "# Seleccionar el dispositivo (GPU si est√° disponible)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Cargar el modelo desde el checkpoint m√°s reciente\n",
    "model_path = './results/checkpoint-2000'  # Aseg√∫rate de cambiar a la ruta correcta\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_path).to(device)\n",
    "\n",
    "# Cargar el tokenizador original (no desde el checkpoint)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Crear un pipeline de clasificaci√≥n\n",
    "classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "# Realizar predicciones\n",
    "reviews = [\"Me encant√≥ la pel√≠cula, fue maravillosa.\", \"No me gust√≥ para nada, muy aburrida.\"]\n",
    "predictions = classifier(reviews)\n",
    "\n",
    "# Imprimir predicciones\n",
    "print(predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# shutil.rmtree('./results/checkpoint-1000')\n",
    "# shutil.rmtree('./results/checkpoint-3000')\n",
    "# shutil.rmtree('./logs')\n",
    "# shutil.rmtree('./results/runs')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cibertec--llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
