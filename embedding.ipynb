{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento de Texto con spaCy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto limpio: ejemplo texto preprocesar\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Cargar el modelo de spaCy para preprocesamiento\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Función de preprocesamiento\n",
    "def preprocess_text(text:str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocesa el texto eliminando stopwords y puntuación, y lematiza las palabras.\n",
    "\n",
    "    Args:\n",
    "        text (str): Texto a preprocesar.\n",
    "\n",
    "    Returns:\n",
    "        str: Texto preprocesado.\n",
    "    \"\"\"\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct])\n",
    "\n",
    "# Texto de ejemplo\n",
    "text = \"Este es un ejemplo de texto para preprocesar.\"\n",
    "cleaned_text = preprocess_text(text)\n",
    "print(\"Texto limpio:\", cleaned_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtener Embeddings con DistilBERT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings: tensor([[[-0.3835, -0.0783, -0.0687,  ..., -0.0525,  0.2811,  0.5119],\n",
      "         [-0.3267, -0.2704,  0.0471,  ...,  0.1809,  0.3451,  0.5138],\n",
      "         [-0.5358, -0.4345,  0.1403,  ...,  0.0510, -0.0980,  0.6037],\n",
      "         ...,\n",
      "         [ 0.2616, -0.1185,  0.2342,  ...,  0.0457, -0.0370,  0.2346],\n",
      "         [-0.3324, -0.0216, -0.1785,  ...,  0.0930, -0.0885,  0.2521],\n",
      "         [ 0.7774,  0.0768, -0.3888,  ...,  0.2486, -0.7300, -0.1771]]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "\n",
    "# Configurar el dispositivo (GPU si está disponible)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Tokenizador y modelo de DistilBERT\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)\n",
    "\n",
    "# Función para obtener embeddings de DistilBERT\n",
    "def get_embeddings(text:str, model:DistilBertModel, tokenizer:DistilBertTokenizer) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Genera los embeddings para el texto dado utilizando el modelo DistilBERT.\n",
    "\n",
    "    Args:\n",
    "        text (str): Texto para el cual generar los embeddings.\n",
    "        model (DistilBertModel): Modelo DistilBERT preentrenado.\n",
    "        tokenizer (DistilBertTokenizer): Tokenizador DistilBERT preentrenado.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Embeddings generados por el modelo DistilBERT.\n",
    "    \"\"\"\n",
    "    encoded_input = tokenizer(text, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoded_input)\n",
    "    return outputs.last_hidden_state\n",
    "\n",
    "# Obtener embeddings para el texto preprocesado\n",
    "embeddings = get_embeddings(cleaned_text, model, tokenizer)\n",
    "print(\"Embeddings:\", embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Calcula las métricas de evaluación para el modelo.\n",
    "\n",
    "    Args:\n",
    "        eval_pred (tuple): Una tupla que contiene logits y etiquetas reales.\n",
    "\n",
    "    Returns:\n",
    "        dict: Un diccionario con la métrica calculada (precisión en este caso).\n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    metric = load_metric(\"accuracy\", trust_remote_code=True)\n",
    "    accuracy = metric.compute(predictions=predictions, references=labels)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga y Evaluación del Modelo sin Fine-Tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\cajue\\anaconda3\\envs\\cibertec--llm\\Lib\\site-packages\\accelerate\\accelerator.py:482: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "100%|██████████| 250/250 [00:03<00:00, 82.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados del modelo sin fine-tuning: {'eval_loss': 0.7018254399299622, 'eval_accuracy': 0.4995, 'eval_runtime': 3.1329, 'eval_samples_per_second': 638.384, 'eval_steps_per_second': 79.798}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer\n",
    "\n",
    "# Cargar el modelo preentrenado sin fine-tuning\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)  # Ajusta el número de etiquetas según tu caso\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Definir los argumentos de entrenamiento (necesarios para el Trainer)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    per_device_eval_batch_size=8,\n",
    "    do_train=False,\n",
    "    do_eval=True,\n",
    ")\n",
    "\n",
    "# Crear un Trainer para evaluar el modelo preentrenado\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics  # Incluir la función de métrica personalizada\n",
    ")\n",
    "\n",
    "# Evaluar el modelo preentrenado\n",
    "baseline_results = trainer.evaluate()\n",
    "print(\"Resultados del modelo sin fine-tuning:\", baseline_results)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Importación de Librerías y Configuración del Logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Configurar el logger para redirigir la salida a un archivo y establecer el nivel de logging a WARNING\n",
    "logging.basicConfig(filename='training_log.txt', level=logging.WARNING)\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Silenciar advertencias específicas de PyTorch\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='torch')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargar y Preprocesar el Dataset de IMDb en Español\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset de IMDb en español\n",
    "data = pd.read_csv('IMDB_Dataset_SPANISH.csv')\n",
    "\n",
    "# Subsamplear el dataset a 20000 muestras\n",
    "data = data.sample(n=20000, random_state=42)\n",
    "\n",
    "# Mapear etiquetas de texto a valores numéricos\n",
    "label_mapping = {\"positivo\": 1, \"negativo\": 0}\n",
    "data['sentimiento'] = data['sentimiento'].map(label_mapping)\n",
    "\n",
    "# Dividir en entrenamiento y validación\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(data['review_es'], data['sentimiento'], test_size=0.2, random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenización\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "# Tokenización\n",
    "tokenizer = BertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts.tolist(), truncation=True, padding=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Definir la clase Dataset\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Seleccionar el dispositivo (GPU si está disponible)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Crear los datasets de entrenamiento y validación\n",
    "train_dataset = Dataset(train_encodings, train_labels.tolist())\n",
    "val_dataset = Dataset(val_encodings, val_labels.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Cargar el Modelo Preentrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuración y Entrenamiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cajue\\anaconda3\\envs\\cibertec--llm\\Lib\\site-packages\\transformers\\training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=2,  # Puedes ajustar según los resultados\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.1,  # Aumentar weight decay para evitar sobreajuste\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=1000,\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    eval_steps=1000,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    disable_tqdm=False\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación de un Callback Personalizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        # Solo imprime logs cada cierto número de pasos\n",
    "        if state.global_step % 1000 == 0:\n",
    "            print(logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cajue\\anaconda3\\envs\\cibertec--llm\\Lib\\site-packages\\accelerate\\accelerator.py:482: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      " 50%|█████     | 1000/2000 [01:07<01:07, 14.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5446, 'grad_norm': 2.492856740951538, 'learning_rate': 3.336666666666667e-05, 'epoch': 1.0}\n",
      "{'loss': 0.5446, 'grad_norm': 2.492856740951538, 'learning_rate': 3.336666666666667e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|█████     | 1000/2000 [01:13<01:07, 14.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4171757698059082, 'eval_accuracy': 0.82175, 'eval_runtime': 5.3641, 'eval_samples_per_second': 745.695, 'eval_steps_per_second': 46.606, 'epoch': 1.0}\n",
      "{'eval_loss': 0.4171757698059082, 'eval_accuracy': 0.82175, 'eval_runtime': 5.3641, 'eval_samples_per_second': 745.695, 'eval_steps_per_second': 46.606, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [02:21<00:00, 14.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3138, 'grad_norm': 5.647911071777344, 'learning_rate': 3.3333333333333334e-08, 'epoch': 2.0}\n",
      "{'loss': 0.3138, 'grad_norm': 5.647911071777344, 'learning_rate': 3.3333333333333334e-08, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2000/2000 [02:27<00:00, 14.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.35327568650245667, 'eval_accuracy': 0.85425, 'eval_runtime': 6.306, 'eval_samples_per_second': 634.321, 'eval_steps_per_second': 39.645, 'epoch': 2.0}\n",
      "{'eval_loss': 0.35327568650245667, 'eval_accuracy': 0.85425, 'eval_runtime': 6.306, 'eval_samples_per_second': 634.321, 'eval_steps_per_second': 39.645, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [02:28<00:00, 13.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 148.0249, 'train_samples_per_second': 216.18, 'train_steps_per_second': 13.511, 'total_flos': 4238956756992000.0, 'train_loss': 0.42916552734375, 'epoch': 2.0}\n",
      "{'train_runtime': 148.0249, 'train_samples_per_second': 216.18, 'train_steps_per_second': 13.511, 'train_loss': 0.42916552734375, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2000, training_loss=0.42916552734375, metrics={'train_runtime': 148.0249, 'train_samples_per_second': 216.18, 'train_steps_per_second': 13.511, 'total_flos': 4238956756992000.0, 'train_loss': 0.42916552734375, 'epoch': 2.0})"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definir el entrenador\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,  # Añadir la función de métrica personalizada\n",
    "    callbacks=[CustomCallback()]  # Mantén esto si tienes callbacks personalizados\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación y Guardado del Modelo Entrenado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:07<00:00, 34.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.35327568650245667, 'eval_accuracy': 0.85425, 'eval_runtime': 7.2679, 'eval_samples_per_second': 550.368, 'eval_steps_per_second': 34.398, 'epoch': 2.0}\n",
      "Resultados de la evaluación: {'eval_loss': 0.35327568650245667, 'eval_accuracy': 0.85425, 'eval_runtime': 7.2679, 'eval_samples_per_second': 550.368, 'eval_steps_per_second': 34.398, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./sentiment_model\\\\tokenizer_config.json',\n",
       " './sentiment_model\\\\special_tokens_map.json',\n",
       " './sentiment_model\\\\vocab.txt',\n",
       " './sentiment_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluación del modelo\n",
    "eval_result = trainer.evaluate()\n",
    "print(f\"Resultados de la evaluación: {eval_result}\")\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "model.save_pretrained('./sentiment_model')\n",
    "tokenizer.save_pretrained('./sentiment_model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicción de Sentimientos con el Modelo Ajustado\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_1', 'score': 0.9876910448074341}, {'label': 'LABEL_0', 'score': 0.9902077317237854}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "import torch\n",
    "\n",
    "# Seleccionar el dispositivo (GPU si está disponible)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Cargar el modelo desde el checkpoint más reciente\n",
    "model_path = './results/checkpoint-2000'  # Asegúrate de cambiar a la ruta correcta\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_path).to(device)\n",
    "\n",
    "# Cargar el tokenizador original (no desde el checkpoint)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Crear un pipeline de clasificación\n",
    "classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "# Realizar predicciones\n",
    "reviews = [\"Me encantó la película, fue maravillosa.\", \"No me gustó para nada, muy aburrida.\"]\n",
    "predictions = classifier(reviews)\n",
    "\n",
    "# Imprimir predicciones\n",
    "print(predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# shutil.rmtree('./results/checkpoint-1000')\n",
    "# shutil.rmtree('./results/checkpoint-3000')\n",
    "# shutil.rmtree('./logs')\n",
    "# shutil.rmtree('./results/runs')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cibertec--llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
